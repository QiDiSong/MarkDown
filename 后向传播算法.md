![image-20220405114326093](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405114326093.png)

若是逐一计算这九个参数的偏导数，计算量太大

![image-20220405114335799](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405114335799.png)

![image-20220405114344801](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405114344801.png)

**为快速计算这九个偏导数，引入后向传播算法（Back Propagation Algorithm）**

**后向传播算法的核心，这九个偏导数是相互关联的**

**根据链式求导法则，可以用一些已经求出偏导数的，去计算那些未求出偏导的，会方便很多**

**因为是从输出，往输入进行推导的，故称作后向传播算法**

先计算离输出较近的偏导数，再计算离输出较远的偏导数

偏导数的计算是由后往前进行的

![image-20220405115523243](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405115523243.png)



![image-20220405120235702](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405120235702.png)

![image-20220405120250812](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405120250812.png)

![image-20220405120335670](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405120335670.png)



**<u>针对多层神经网络的一般情况进行推导：</u>**

不限制神经网络的层数和每一层神经元的个数

![image-20220405120821298](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405120821298.png)

![image-20220405120916379](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405180443171.png)

SGD随机梯度下降解决的问题：

参数更新，每次输入一个训练样本就更新一次参数，但是在实际应用中，这样存在严重的问题

1. 如果每输入一个训练数据，就更新所有的训练参数，这样训练速度就会非常慢
2. 如果梯度的更新，只依赖一个梯度的数据，那么这个训练数据带来的误差，将会传导到每一个参数中去。单一数据带来的随机性是非常大的。这样会使算法收敛变得非常缓慢

![image-20220405180938686](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405180938686.png)

SGD思想：将训练样本每batch size个数据划分为一个batch

每一个batch更新一次参数

![image-20220405182422304](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405182422304.png)

训练完一次所有的训练数据称为一个EPOCH

要训练多个EPOCH，保证打乱训练样本的次序，增加训练样本的随机性

![image-20220405182603721](C:\Users\33010\AppData\Roaming\Typora\typora-user-images\image-20220405182603721.png)