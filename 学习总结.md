1. aipuparse用法：

​    写一个parse.cfg文件，里面格式如下：

​    之后，使用命令aipuparse --cfg parse.cfg，该命令可以跑对应格式的parse过程。

2. 什么是超参数，参数和超参数的区别？
   
      区分两者最大的一点就是是否通过数据来进行调整，**模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数**。例如卷积核的具体核参数就是指模型参数，这是有数据驱动的。**而学习率则是人为来进行调整的超参数**。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。

​        超参数可以看做**不是从数据中学习的参数**，**而是模型调节旋钮**，可以来回旋转调整模型的性能。

3. 增量式编译/复制
   
   增量式编译会查看文件的时间戳，若有最新的文件，则编译最新的文件，原有文件保留不变化，节约编译时间
   
   增量式复制：原有文件在目标文件下已存在，则已存在的文件，不会再复制。

4. 标定数据集一般是从测试数据集里面抽出100张图片作为标定数据集（Calibration dataset）

5. 前面介绍了通过使用tf.train.Saver函数来保存TensorFlow程序的参数，但是，在使用tf.train.Saver函数保存模型文件的时候，是保存所有的参数信息，而有些时候我们并不需要所有的参数信息。我们只需要知道神经网络的输入层经过前向传播计算得到输出层即可，所以在保存的时候，我们也不需要保存所有的参数，以及变量的初始化、模型保存等辅助节点信息与迁移学习类似。之前使用tf.train.Saver函数保存模型文件的时候会产生多个文件，它将变量的取值和计算图结构分成了不同的文件存储。TensorFlow提供了另一种保存模型文件的方法，将计算图保存在一个文件中。
   
   **1、模型文件的保存**

```python
import tensorflow as tf
from tensorflow.python.framework import graph_util
from tensorflow.python.platform import gfile

if __name__ == "__main__":
    a = tf.Variable(tf.constant(5.,shape=[1]),name="a")
    b = tf.Variable(tf.constant(6.,shape=[1]),name="b")
    c = a + b
    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)
    #导出当前计算图的GraphDef部分
    graph_def = tf.get_default_graph().as_graph_def()
    #保存指定的节点，并将节点值保存为常数
    output_graph_def = graph_util.convert_variables_to_constants(sess,graph_def,['add'])
    #将计算图写入到模型文件中
    model_f = tf.gfile.GFile("model.pb","wb")
    model_f.write(output_graph_def.SerializeToString())
```

   convert_variables_to_constants函数，会将计算图中的变量取值以常量的形式保存。在保存模型文件的时候，我们只是导出了GraphDef部分，GraphDef保存了从输入层到输出层的计算过程。在保存的时候，通过convert_variables_to_constants函数来指定保存的 **节点名称**而不是 **张量的名称**， **“add:0”**是张量的名称而 **"add"**表示的是节点的名称。

   **2、模型文件的读取**

```python
 sess = tf.Session()
 #将保存的模型文件解析为GraphDef
 model_f = gfile.FastGFile("model.pb",'rb')
 graph_def = tf.GraphDef()
 graph_def.ParseFromString(model_f.read())
 c = tf.import_graph_def(graph_def,return_elements=["add:0"])
 print(sess.run(c))
 #[array([ 11.], dtype=float32)]
```

   在读取模型文件获取变量的值的时候，我们需要指定的是 **张量的名称**而不是 **节点的名称**，这两个地方需要特别注意一下。在读取模型文件的时候，可能会遇到一个错误 **tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: ./model.pd**，打开模型文件的时候报错，所以一般都是模型文件的名称有问题，要注意统一。在这个地方，我遇到了一个很奇葩的问题，我读取模型文件的名称和保存的文件名称是一样，但是还是报错，后面我将保存模型文件的名称复制到读取模型文件的名称那里，就解决了这个问题。

3. 全连接层作用

****全连接层**（**fully connected layers, FC)在整个卷积神经网络中起到”分类器“的作用。如果说卷积层，池化层和[激活函数](https://so.csdn.net/so/search?q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&spm=1001.2101.3001.7020)层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的”分布式特征表示“映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现；

对前层是全连接的全连接层可以转化为卷积核为1*1的卷积，而前层是卷积层的全连接层可以转化为卷积核为h*w的全局卷积，h和w分别为前层卷积结果的高和宽。

全连接的核心操作就是矩阵向量乘积y=Wx

本质就是由一个特征空间线性变换到另一个特征空间。目标空间中的任一维——也就是隐层的一个cell——都认为会受到源空间的每一维的影响。不考虑严谨，可以说，目标向量是源向量的加权和。

在CNN中，全连接常出现在最后几层，用于对于前面设计的特征做加权和，比如mnist，前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。（卷积相当于全连接的有意弱化，按照局部视野的启发，把局部之外的弱影响直接抹为0影响，还做了一点强制，不同的局部所使用的参数居然一致。弱化使参数变少，节省计算量，又专攻局部不贪多求全，强制进一步减少参数。在RNN中，全连接用来把embedding空间拉到隐层空间，把隐层空间转回label空间等。

全连接层的作用主要就是实现分类（Classification）  
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210306173300357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RqZmprajUy,size_16,color_FFFFFF,t_70)![在这里插入图片描述](https://img-blog.csdnimg.cn/20210306173446348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RqZmprajUy,size_16,color_FFFFFF,t_70)  
还是就这张图而言，经过若干卷积层和池化层之后，会输出20个12*12的矩阵（是矩阵，不是图片，虽然可以可视化，但一般已经没有人能理解的部分在了），这代表用了20个神经元，每个神经元都对这张图进行了一次次卷积，并且每个神经元最后输出的12*12的矩阵都代表了这个神经元对该图片的一个特征的理解接下来就到了全连接层，输出一个1*100的矩阵，全连接层是100个12*12的卷积核，每个12*12的卷积核都对这20个12*12的图片进行卷积，得到20个数，将这20个数求和得到矩阵中的一个数，一共100个卷积核，所以一共是100个数，而这100个数，代表着用于分类的最基本特征，比如有耳朵，有尾巴，有眼睛…（只是比如）最后再经过一个分类器（也是一个全连接层？），假设要分猫，狗，兔子，那么对这些特征再进行一次计算（降维），将其中的某几个特征求和，输出最后的1*3的矩阵，每个数代表了各个类别的概率（或得分）

4. tf搭建框架的三种方法和区别
- tf.nn：最底层的函数，其他各种库可以说都是基于这个底层库来进行扩展的。
- tf.layers：比tf.nn更高级的库，对tf.nn进行了多方位功能的扩展。用程序员的话来说，就是用tf.nn造的轮子。**最大的特点就是库中每个函数都有相应的类（函数名为大写,看了下底层源码，是从kears那迁移过来的）**。
- tf.keras：如果说tf.layers是轮子，那么keras可以说是汽车。tf.keras是基于tf.layers和tf.nn的高度封装。

每个库有每个库的优势，tf.nn和tf.layers灵活性相对较高，如果需要对底层进行很多操作，这两个库的优势就会很明显。但是如果工业上想搭建已经很成熟的网络并进行训练，那么keras相对来说会更加容易一些。具体需要在实际运用中去体会。 **综上所述，这几种库都可以运用，都很不错，具体看运用背景。**

5. 