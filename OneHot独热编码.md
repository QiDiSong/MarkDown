在机器学习算法中，常会遇到分类特征是离散的，无序的。例如：性别有男、女，城市有北京，上海，深圳等。

> 性别特征：  
> ["男"，"女"] => 0，1  
> 地区特征：  
> ["北京"，"上海，"深圳"] => 0，1，2  
> 工作特征：  
> ["演员"，"厨师"，"公务员"，"工程师"，"律师"] => 0，1，2，3，4

比如，样本(女，北京，工程师)=>(1，0，3)，但是，这样的特征处理并不能直接放入机器学习算法中，因为，分类器通常数据是连续且有序。解决这类问题，一种解决方法是采用独热编码（One-Hot Encoding）。

#### 什么是独热编码

独热编码（One-Hot Encoding），又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。即，只有一位是1，其余都是零值。

> 例如，对六个状态进行编码：  
> 自然顺序码为 000,001,010,011,100,101  
> 独热编码则是 000001,000010,000100,001000,010000,100000

回到一开始的例子，性别特征：["男","女"]，按照N位状态寄存器来对N个状态进行编码的原理：

> 性别特征：["男"，"女"]（这里N=2）  
> 男 => 10  
> 女 => 01

> 地区特征：["北京"，"上海，"深圳"]（这里N=3）：  
> 北京 => 100  
> 上海 => 010  
> 深圳 => 001

> 工作特征：["演员"，"厨师"，"公务员"，"工程师"，"律师"]（这里N=5）：  
> 演员 => 10000  
> 厨师 => 01000  
> 公务员 => 00100  
> 工程师 => 00010  
> 律师 => 00001

所以，样本的特征是["女","北京","工程师"]的时候，独热编码（One-Hot Encoding）的结果为：

> [0，1，1，0，0，0，0，0，1，0]

#### 为什么要进行独热编码

在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的。而常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。  
使用独热编码（One-Hot Encoding），将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用独热编码（One-Hot Encoding），会让特征之间的距离计算更加合理。  
比如，上面的工作特征，该离散型特征，共有五个取值，不使用独热编码（One-Hot Encoding），其表示分别是：

> 演员 = (0)  
> 厨师 = (1)  
> 公务员 = (2)  
> 工程师 = (3)  
> 律师 = (4)

两个工作之间的距离是：

> d(演员，厨师) = 1  
> d(厨师，公务员) = 1  
> d(公务员，工程师) = 1  
> d(工程师，律师) = 1  
> d(演员，公务员） = 2  
> d(演员，工程师） = 3  
> .....

显然这样的表示，计算出来的特征的距离是不合理。那如果使用独热编码（One-Hot Encoding），则得到d(演员，厨师) = 1与d(演员，公务员）都是1。那么，两个工作之间的距离就都是sqrt(2)。即每两个工作之间的距离是一样的，显得更合理。

#### 什么情况下不需要独热编码

1、如果特征是离散的，并且不用独热编码就可以很合理的计算出距离，就没必要进行独热编码。（比如，离散特征共有1000个取值，分成两组是400和600,两个小组之间的距离有合适的定义，组内距离也有合适的定义，就没必要独热编码）  
2、有些并不是基于向量空间度量的算法，数值只是个类别符号，没有偏序关系，就不用进行独热编码。  
3、如果原本的标签编码是有序的，就不必独热编码了，因为会丢失顺序信息。

作者：李春辉  
链接：https://www.jianshu.com/p/42e93acacc52  
来源：简书  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
